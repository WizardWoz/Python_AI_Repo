{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算图（实质是有向无环图DAG）\n",
    "# 1.将代码分解成操作子\n",
    "# 2.将计算表示成一个无环图：前向传播执行图并存储中间结果（求复合函数值），反向传播执行图并计算梯度、去除不需要的枝（求偏导数和梯度）\n",
    "import numpy as np\n",
    "if not hasattr(np, 'bool'):\n",
    "    np.bool = bool  # monkey-patch for deprecated np.bool usage\n",
    "\n",
    "# 显式构造计算图（Tensorflow、Theano、MXNet）\n",
    "from mxnet import sym\n",
    "a=sym.var('a')  # 变量\n",
    "b=sym.var('b')  # 变量\n",
    "c=2*a+b  # 操作子（计算图的节点）\n",
    "# 隐式构造计算图（PyTorch、MXNet）\n",
    "from mxnet import autograd,nd\n",
    "with autograd.record():\n",
    "    a=nd.ones((2,1))\n",
    "    b=nd.ones((2,1))\n",
    "    c=2*a+b\n",
    "# 微分求导链式法则；正向传播即计算图的前向遍历（从最右微分求导到最左微分求导），反向传播即计算图的后向遍历（从最左微分求导到最右微分求导）\n",
    "# 平时高数练习题用的就是反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcbc06c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor(6., grad_fn=<SumBackward0>)\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 自动求导\n",
    "import torch\n",
    "x=torch.arange(4.0)\n",
    "print(x)\n",
    "\n",
    "# 计算y关于x的梯度之前，需要一个地方来存储梯度\n",
    "x.requires_grad_(True)  # 设置x为需要梯度的变量，等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # x的梯度，初始值为None；y关于x的梯度即存放于此\n",
    "y=2*torch.dot(x,x)  # y是x的函数，即y=2*x1^2+2*x2^2+2*x3^2+2*x4^2\n",
    "print(y) # y=2*14=28，因为之前设置x为需要梯度的变量，所以隐式构造计算图，有关梯度的函数存放于grad_fn\n",
    "# 通过调用反向传播函数来自动计算y关于x每个分量的梯度\n",
    "y.backward()    # y对每个分量的梯度dy/dx_i存放于x.grad中\n",
    "print(x.grad)  # 输出y对每个分量的梯度dy/dx_i，即dy/dxi\n",
    "\n",
    "# 默认情况下，PyTorch会累加梯度到x.grad中，因此每次调用y.backward()前需清除旧值\n",
    "x.grad.zero_()  # 清除梯度\n",
    "y=x.sum()  # y是x的函数，即y=x1+x2+x3+x4\n",
    "print(y)  # y=6，因为之前设置x为需要梯度的变量，所以隐式构造计算图，有关梯度的函数存放于grad_fn\n",
    "y.backward()  # 计算y关于x的梯度\n",
    "print(x.grad)  # 输出y对每个分量的梯度dy/dx_i，即dy/dxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beaa2902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将某些计算移动到记录的计算图之外\n",
    "x.grad.zero_()  # 清除梯度\n",
    "y=x*x  # y是x的函数，即y=xTx\n",
    "u=y.detach()  # u是x的函数，但不需要梯度\n",
    "z=u*x  # z是x的函数，即z=x1^3+x2^3+x3^3+x4^3\n",
    "z.sum().backward()  # 计算z关于x的梯度，实际z关于x的梯度是u\n",
    "x.grad==u  # 输出True，说明u的梯度没有被计算出来\n",
    "\n",
    "# 不带参数的tensor.backward()假设tensor是一个标量\n",
    "x.grad.zero_()  # 清除梯度\n",
    "y.sum().backward()  # 计算y关于x的梯度\n",
    "x.grad==2*x  # 输出True，说明y关于x的梯度是2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05049630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即使构建函数的计算图需要经过Python控制流（如循环、条件分支、函数调用等），PyTorch仍然可以计算得到变量的梯度\n",
    "def f(a):\n",
    "    b=a*2\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c=b\n",
    "    else:\n",
    "        c=100*b\n",
    "    return c\n",
    "a=torch.randn(size=(),requires_grad=True)\n",
    "d=f(a)\n",
    "d.backward()\n",
    "a.grad==d/a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
